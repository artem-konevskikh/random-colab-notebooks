{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WcF2V6NkK4A_0FqbfIo_0lD9SQ1fRIxw",
      "authorship_tag": "ABX9TyO/ABH0s7mCD9aZlK4TcUX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artem-konevskikh/random-colab-notebooks/blob/main/rugpt3_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RuGPT3 Finetuning and text generation\n",
        "\n",
        "Made by [Artem Konevskikh](https://aiculedssul.net)"
      ],
      "metadata": {
        "id": "3PXW57vv-yHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install transformers\n",
        "!pip install transformers \n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "import torch\n",
        "DEVICE = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YDkztNq5-xNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive\n",
        "#@markdown Mount Google Drive to save/load finetuned models\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LAa2S2-rEqtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "EPh6BlU2D1nE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning\n",
        "\n",
        "You can skip this part if you already have your model"
      ],
      "metadata": {
        "id": "_tb9G1V0D6A_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lCvMbMd7-wh4"
      },
      "outputs": [],
      "source": [
        "#@title Load RuGPT3-small model\n",
        "model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare dataset\n",
        "dataset_path = \"/content/dataset.txt\" #@param {\"type\": \"string\"}\n",
        "\n",
        "train_dataset = TextDataset(tokenizer=tokenizer,file_path=dataset_path,block_size=64)  \n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VsgUuZBW_Rjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Params\n",
        "#@markdown The output directory where model will be saved (you can store it on the drive to reuse it later)\n",
        "model_dir = \"/content/drive/MyDrive/AI/rugpt3\" #@param {\"type\": \"string\"}\n",
        "#@markdown Overwrite the content of the output directory\n",
        "overwrite_output_dir=True #@param {\"type\": \"boolean\"}\n",
        "#@markdown Number of training epochs\n",
        "num_train_epochs=80 #@param {\"type\": \"integer\"}\n",
        "#@markdown Batch size for training\n",
        "per_device_train_batch_size=32 #@param {\"type\": \"integer\"}\n",
        "#@markdown Batch size for evaluation\n",
        "per_device_eval_batch_size=32 #@param {\"type\": \"integer\"}\n",
        "#@markdown Number of warmup steps for learning rate scheduler\n",
        "warmup_steps=10 #@param {\"type\": \"integer\"}\n",
        "#@markdown To make \"virtual\" batch size larger\n",
        "gradient_accumulation_steps=16 #@param {\"type\": \"integer\"}\n",
        "#@markdown Learning rate (set smaller learning rate for smaller datasets)\n",
        "lr = 0.00001 #@param {type:\"slider\", min:1e-5, max:1e-4, step:4.5e-5}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output/\", #The output directory\n",
        "    overwrite_output_dir=overwrite_output_dir, #overwrite the content of the output directory\n",
        "    num_train_epochs=num_train_epochs, # number of training epochs\n",
        "    per_device_train_batch_size=per_device_train_batch_size, # batch size for training\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,  # batch size for evaluation\n",
        "    warmup_steps=warmup_steps,# number of warmup steps for learning rate scheduler\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps, # to make \"virtual\" batch size larger\n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=lr),None) # Optimizer and lr scheduler\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "K5Wb7F3E_9_z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Finetuning\n",
        "#@markdown This will run the finetuning and save the model after that\n",
        "trainer.train()\n",
        "trainer.save_model(model_dir)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bce8_V1rClsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "h1jX8p21DmQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate with finetuned model"
      ],
      "metadata": {
        "id": "W75smQPeDoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load finetuned model\n",
        "#@markdown The directory where finetuned model is stored\n",
        "model_dir = \"/content/drive/MyDrive/AI/rugpt3\" #@param {\"type\": \"string\"}\n",
        "\n",
        "\n",
        "model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_dir).to(DEVICE)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WzMilBZKDueF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate\n",
        "#@markdown Prompt to continue\n",
        "text = '' #@param {\"type\": \"string\"}\n",
        "#@markdown Max length of the generated text\n",
        "max_length = 100 #@param {\"type\": \"integer\"}\n",
        "#@markdown Temperature. Best results in range 0.8-2\n",
        "temperature = 0.8  #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "out = model.generate(input_ids, do_sample=True, temperature=1.3, max_length=30)\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print(generated_text)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mBhXf6KlN6n7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}